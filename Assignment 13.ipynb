{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d807ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb347bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --no-cache-dir spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('C:/Users/tarmc/Downloads/Gwen_Walz_Article.pdf')\n",
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Open the PDF file\n",
    "with open('C:/Users/tarmc/Downloads/Gwen_Walz_Article.pdf', 'rb') as file:\n",
    "    reader = pypdf.PdfReader(file)\n",
    "    text = ''\n",
    "    for page_number in range(len(reader.pages)):\n",
    "        text += reader.pages[page_number].extract_text()\n",
    "\n",
    "# Tokenize the text and count the occurrences of each word\n",
    "words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Show the most common words\n",
    "most_common_words = word_counts.most_common(10)  # Change 10 to the desired number of most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb43e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "import nltk\n",
    "# Download the required nltk data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Open the PDF file\n",
    "with open('C:/Users/tarmc/Downloads/Gwen_Walz_Article.pdf', 'rb') as file:\n",
    "    reader = pypdf.PdfReader(file)\n",
    "    text = ''\n",
    "    for page_number in range(len(reader.pages)):\n",
    "        text += reader.pages[page_number].extract_text()\n",
    "\n",
    "# Tokenize the text and perform part-of-speech tagging\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tagged_words = [nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "# Filter words based on part of speech and count the occurrences of each word\n",
    "noun_counts = Counter(word.lower() for sentence in tagged_words for word, tag in sentence if tag.startswith('N'))\n",
    "\n",
    "# Show the most common nouns\n",
    "most_common_nouns = noun_counts.most_common(10)  # Change 10 to the desired number of most common nouns\n",
    "print(most_common_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "import spacy\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"They married in 1994 and share two children Hope and Gus.\"\n",
    "\n",
    "# Process the sentence using spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract subject/object relationships\n",
    "subject_object_pairs = []\n",
    "for token in doc:\n",
    "    if token.dep_ in [\"nsubj\", \"dobj\"]:  # nsubj: nominal subject, dobj: direct object\n",
    "        subject_object_pairs.append((token.head.text, token.text, token.dep_))\n",
    "\n",
    "print(subject_object_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8faf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Open the PDF file\n",
    "with open('C:/Users/tarmc/Downloads/Gwen_Walz_Article.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page_number in range(len(reader.pages)):\n",
    "        text += reader.pages[page_number].extract_text()\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities and their types\n",
    "entity_types = Counter((ent.text, ent.label_) for ent in doc.ents)\n",
    "\n",
    "# Show the most common entities and their types\n",
    "most_common_entities = entity_types.most_common(10)  # Change 10 to the desired number of most common entities\n",
    "print(most_common_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ff3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Open the PDF file\n",
    "with open('Gwen_Walz_Article.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page_number in range(len(reader.pages)):\n",
    "        text += reader.pages[page_number].extract_text()\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities and their dependencies\n",
    "entity_dependencies = [(ent.text, ent.label_, ent.root.head.text, ent.root.head.dep_) for ent in doc.ents]\n",
    "\n",
    "# Show the entities and their dependencies\n",
    "print(entity_dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Using the medium-sized model with word vectors\n",
    "\n",
    "# Open the PDF file\n",
    "with open('Gwen_Walz_Article.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page_number in range(len(reader.pages)):\n",
    "        text += reader.pages[page_number].extract_text()\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find the most similar words\n",
    "target_word = \"climate\"  # Change to the target word for which you want to find similar words\n",
    "similar_words = []\n",
    "for token in doc:\n",
    "    if token.is_alpha and token.text.lower() != target_word:  # Exclude the target word itself\n",
    "        similarity = nlp(target_word).similarity(token)\n",
    "        similar_words.append((token.text, similarity))\n",
    "\n",
    "# Sort the similar words by similarity score\n",
    "similar_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show the most similar words\n",
    "print(similar_words[:10]) # Change 10 to the desired number of most similar words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
